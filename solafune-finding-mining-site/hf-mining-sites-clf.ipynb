{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":7684079,"sourceType":"datasetVersion","datasetId":4483500},{"sourceId":26086,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":16846}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:15:58.434800Z","iopub.execute_input":"2024-04-09T06:15:58.435441Z","iopub.status.idle":"2024-04-09T06:16:02.019341Z","shell.execute_reply.started":"2024-04-09T06:15:58.435399Z","shell.execute_reply":"2024-04-09T06:16:02.018374Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport time\nimport math\nfrom tifffile import tifffile as tif\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.utils.data as data\n\nimport torchvision\nfrom torchvision.transforms import v2\nfrom torchvision.io import read_image\n\nimport evaluate\nimport transformers\nfrom transformers import TrainingArguments, Trainer\nfrom transformers.models.convnext.modeling_convnext import ConvNextForImageClassification\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport matplotlib.pyplot as plt\nimport wandb\n\n\ntorch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T06:16:02.021052Z","iopub.execute_input":"2024-04-09T06:16:02.021334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineering(image):\n    \"\"\"\n    Normalized Difference Vegetation Index: NDVI = (NIR - R) / (NIR + R)\n    Normalized Difference Water Index: NDWI = (NIR - G) / (NIR + G)\n    Normalized Difference Moisture Index: NDMI = (NIR - SWIR_1) / (NIR + SWIR_1)\n    Enhanced Vegetation Index: EVI = 2.5 * (NIR - RED) / ((NIR + 6 * RED - 7.5 * BLUE) + 1)\n\n    \"\"\"\n    endmi = ((image[..., 7] + image[..., 8]) - (image[..., 10] + image[..., 11])) / (image[..., 7] + image[..., 8] + image[..., 10] + image[..., 10] + image[..., 11] + 1e-10)\n    ndvi = (image[..., 7] - image[..., 3]) / (image[..., 7] + image[..., 3] + 1e-10)\n    ndmi = (image[..., 7] - image[..., 10]) / (image[..., 7] + image[..., 10] + 1e-10)\n    ndwi = (image[..., 7] - image[..., 2]) / (image[..., 7] + image[..., 2] + 1e-10)\n    image = np.concatenate(\n        [\n            image, # scale band data from -1 to 1\n            np.expand_dims(endmi, axis=-1),\n            np.expand_dims(ndvi, axis=-1),\n            np.expand_dims(ndmi, axis=-1),\n            np.expand_dims(ndwi, axis=-1),\n        ],\n        axis=-1,\n    )\n    return image\n\nclass MSCLF(data.Dataset):\n    def __init__(self, cfg,train=True,transforms=None, msclf=None):\n        self.data_path = cfg['data_path']\n        self.csv_file = cfg['train_set']\n        self.fe = cfg['feature_engineering']\n        self.train_split = cfg['train_split']\n        \n        self.msclf = msclf\n        self.transforms = transforms\n        self.train = train\n        self.band_scalers = torch.Tensor([ \n            1.2680000066757202,1.5047999620437622,                               \n            1.5046600103378296,1.511856198310852,                               \n            1.5050400495529175,1.5010639429092407,                               \n            1.5003302097320557,1.5195592641830444,                               \n            1.49590003490448,1.5194000005722046,                               \n            1.1084500551223755,1.2992000579833984\n        ]).unsqueeze(-1).unsqueeze(-1)\n        \n        if msclf is None:\n            self.msclf = pd.read_csv(self.csv_file)\n            if self.train_split != None:\n                if self.train:\n                    self.msclf = self.msclf[:math.ceil(len(self.msclf) * self.train_split)]\n                else:\n                    self.msclf = self.msclf[math.ceil(len(self.msclf) * self.train_split):]\n                    self.msclf.index = [i for i in range(len(self.msclf))] \n                    \n        labels_weight = compute_class_weight(class_weight=\"balanced\",\n                         classes=pd.unique(self.msclf.iloc[:,1]),\n                         y=self.msclf.iloc[:,1])\n        self.labels_weight = torch.tensor(\n           [labels_weight[1],labels_weight[0]] # index 0 is positive sample\n        )\n    def __len__(self):\n        return len(self.msclf)\n    \n    def set_transforms(self, transforms):\n        self.transforms = transforms\n        \n    def __getitem__(self, idx):\n        image, label = self.msclf.iloc[idx,0],self.msclf.iloc[idx,1]\n        image = tif.imread(os.path.join(self.data_path,image))\n        image *= 2 - 1\n        if self.fe:\n            image = feature_engineering(image)\n        label = np.stack([label,1-label])\n        image = torch.from_numpy(image)\n        label = torch.from_numpy(np.array(label))\n        label = label.to(torch.float32)\n        image = torch.transpose(image,0,2)  \n        if self.transforms:\n            image, label = self.transforms(image,label)\n        return {'pixel_values':image, 'label':label}\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ds(cfg):\n    train_ds = MSCLF(cfg,train=True)\n    val_ds = MSCLF(cfg,train=False)\n    return train_ds,val_ds\n\ndef compute_metrics(eval_pred):\n    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n    predictions = np.argmax(eval_pred.predictions, axis=1)\n    label_ids = np.argmax(eval_pred.label_ids, axis=1)\n    return clf_metrics.compute(predictions=predictions, references=label_ids)\n\ndef collate_fn(examples):\n#     print([example['label'] for example in examples])\n    pixel_values = torch.stack([example['pixel_values'] for example in examples])\n    labels = torch.stack([example['label'] for example in examples])\n    return {'pixel_values':pixel_values, 'labels':labels}\n\nclass MSCLFTrainer(Trainer):\n    def set_labels_weight(self, labels_weight):\n        self.labels_weight = labels_weight\n    \n    def compute_loss(self, model, inputs, return_outputs=False):\n        device = model.device if (\n            isinstance(model == ConvNextForImageClassification)\n        ) else model.module.device\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        probs = torch.nn.functional.softmax(logits,dim=1)\n        loss_fct = torch.nn.CrossEntropyLoss(\n            weight=self.labels_weight.to(device),\n            label_smoothing=0.1\n        )\n        loss = loss_fct(probs.view(-1, self.model.config.num_labels), labels)\n        return (loss, outputs) if return_outputs else loss\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nCFG = {\n    'data_path':'/kaggle/input/finding-mining-sites/train/train',\n    'train_set':'/kaggle/input/finding-mining-sites/train/answer.csv',\n    'feature_engineering':True,\n    'train_split':None,\n    'input_size':(256,256),\n    'model_type':'ConvNeXt-B',\n}\nconvnext_type = {\n    \"ConvNeXt-T\": {\"hidden_sizes\": (96, 192, 384, 768), \"depths\": (3, 3, 9, 3)},\n    \"ConvNeXt-S\": {\"hidden_sizes\": (96, 192, 384, 768), \"depths\": (3, 3, 27, 3)},\n    \"ConvNeXt-B\": {\"hidden_sizes\": (128, 256, 512, 1024), \"depths\": (3, 3, 27, 3)},\n    \"ConvNeXt-L\": {\"hidden_sizes\": (192, 384, 768, 1536), \"depths\": (3, 3, 27, 3)},\n    \"ConvNeXt-XL\":{\"hidden_sizes\": (256, 512, 1024, 2048), \"depths\": (3, 3, 27, 3)},\n}\n\n\nconvnext_type = \n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=69)\ndf = pd.read_csv(CFG['train_set'])\n\ntrain_transforms = v2.Compose([\n    v2.RandomChoice([\n        v2.RandomResizedCrop(CFG['input_size'],antialias=True),\n        v2.Resize(CFG['input_size'],antialias=True),\n        v2.RandomCrop(CFG['input_size']),\n        v2.Compose([\n            v2.RandomRotation(degrees=45),\n            v2.CenterCrop(CFG['input_size']),\n        ]),\n    ]),\n    v2.RandomHorizontalFlip(),\n    v2.RandomVerticalFlip(), \n])\nval_transforms = v2.Compose([\n        v2.Resize(CFG['input_size'],antialias=True),\n])\n\nclf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n\n\nfor i, (train_index, val_index) in enumerate(skf.split(df.iloc[:,0],df.iloc[:,1].copy())):\n    train_msclf, val_msclf = df.iloc[train_index,:], df.iloc[val_index,:] \n    train_ds, val_ds = MSCLF(CFG, msclf=train_msclf), MSCLF(CFG, msclf=val_msclf)\n\n    train_ds.set_transforms(train_transforms)\n    val_ds.set_transforms(val_transforms)\n\n\n    model_config = transformers.ConvNextConfig(\n        num_channels=16,\n        num_labels=2,\n        hidden_sizes=convnext_type[CFG['model_type']]['hidden_sizes'],\n        depths=convnext_type[CFG['model_type']]['depths'],\n    )\n\n    model = transformers.ConvNextForImageClassification(model_config)\n\n    training_args = TrainingArguments(\n        f'msclf{i}',\n        overwrite_output_dir=False,\n        remove_unused_columns=False,\n        evaluation_strategy = 'epoch',\n        save_strategy = \"epoch\",\n        learning_rate=9e-5,\n        per_device_train_batch_size=700,\n        per_device_eval_batch_size=300,\n        gradient_accumulation_steps=4,\n        auto_find_batch_size=True,\n        num_train_epochs=1000,\n        warmup_ratio=0.1,\n        logging_strategy='epoch', \n        save_total_limit=2,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"f1\",\n        push_to_hub=False,\n    )\n    trainer = MSCLFTrainer(\n        model,\n        training_args,\n        train_dataset=train_ds,\n        eval_dataset=val_ds,\n        compute_metrics=compute_metrics,\n        data_collator=collate_fn,\n    )\n    trainer.train(resume_from_checkpoint=True)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}